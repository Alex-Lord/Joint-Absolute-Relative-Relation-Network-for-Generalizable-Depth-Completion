
import sys
sys.path.extend(['/data1/Chenbingyuan/Depth-Completion/src'])
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.nn import Module, Conv2d, init, Sequential, ModuleDict
from custom_blocks import *

from .modules import FirstModule, UNetModule, StackedBottleNeck, Encoder, Decoder
from einops import rearrange
from torch.cuda.amp import autocast
from .utils import save_feature_as_uint8colored
from .cby_unet import *

from .baselines.BPnet.models.BPNet import Net as BPnetModel

rgb_x_chan = [512, 512, 512, 512, 256, 128, 64]

sf_chan = [512, 512, 512, 512, 256, 128, 64]

class BPNet(Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.BPmodules = BPnetModel()
        fx, fy, cx, cy = 582.6244, 582.6910, 313.0447, 238.4438
        self.K = torch.Tensor([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])
        self.K = self.K.unsqueeze(0)
        self.KITTI_factor = 80
        for m in self.modules():
            if isinstance(m, Conv2d):
                init.kaiming_normal_(m.weight, a=0.2)
                if m.bias is not None:  # ðŸ”¥ é¿å…å¯¹ None æ‰§è¡Œ zero_()
                    init.zeros_(m.bias)

    @autocast()         
    def forward(self,
        rgb: Tensor,
        point: Tensor,
        hole_point: Tensor, 
    ) -> Tensor:
        # èŽ·å– point æ‰€åœ¨çš„è®¾å¤‡
        device = point.device
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥è½¬ç§»åˆ°åŒä¸€è®¾å¤‡
        point = point * self.KITTI_factor  # å‡è®¾ KITTI_factor æ˜¯æ ‡é‡æˆ–å·²åœ¨ device ä¸Š
        k = self.K
        # å¦‚æžœè®¾å¤‡ä¸åŒï¼Œæ‰‹åŠ¨å°†å®ƒä»¬ç§»åˆ°ç›¸åŒè®¾å¤‡
        device = rgb.device  # å‡è®¾ rgb å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        point = point.to(device)
        k = k.to(device)

        gen_depth = self.BPmodules(rgb, point, k)
        return (gen_depth,) * 5
# JARRN
# class UNet(Module):
#     def __init__(self, rgb_x_layer_num: int = 7, rezero: bool = False):
#         super(UNet, self).__init__()
#         # rgb+x channel
#         layer = FirstModule(rgb_x_chan[1], rgb_x_chan[0], rezero)
#         for i in range(2, rgb_x_layer_num):
#             layer = UNetModule(layer, rgb_x_chan[i], rgb_x_chan[i - 1], rezero)

#         self.rgb_x_block = Sequential(
#             Conv2d(5, rgb_x_chan[i], 3, 1, 1),
#             StackedBottleNeck(rgb_x_chan[i], rgb_x_chan[i], rezero),
#             layer,
#             StackedBottleNeck(2 * rgb_x_chan[i], rgb_x_chan[i], rezero),
#             Conv2d(rgb_x_chan[i], 1, 3, 1, 1),
#         )
        
#         # sfmap channel
#         sf_layer = FirstModule(sf_chan[1], sf_chan[0], rezero)
#         for i in range(2, rgb_x_layer_num):
#             sf_layer = UNetModule(sf_layer, sf_chan[i], sf_chan[i - 1], rezero)

#         # # ä½¿ç”¨G2æ—¶æ³¨é‡Š
#         self.sfmap_block = Sequential(
#             Conv2d(3, sf_chan[i], 3, 1, 1),
#             StackedBottleNeck(sf_chan[i], sf_chan[i], rezero),
#             sf_layer,
#             StackedBottleNeck(2 * sf_chan[i], sf_chan[i], rezero),
#             Conv2d(rgb_x_chan[i], 3, 3, 1, 1),  #åŽŸå§‹sfv2
#             # Conv2d(rgb_x_chan[i], 2, 3, 1, 1),  #ä¸ç”¨Probï¼Œsæˆ–f
#         )
        
#         self.softmax = nn.Softmax(dim=1)
        
#         # initializing
#         for m in self.modules():
#             if isinstance(m, Conv2d):
#                 init.kaiming_normal_(m.weight, a=0.2)
#                 init.zeros_(m.bias)
        
 
#     @autocast()
#     def forward(
#         self,
#         rgb: Tensor,
#         point: Tensor,
#         hole_point: Tensor, 
#     ) -> Tensor:

#         x1 = torch.cat((rgb, point, hole_point), dim=1)
#         depth = self.rgb_x_block(x1)


        
#         # # ä½¿ç”¨G2æ—¶æ³¨é‡Š
#         sfmap = self.sfmap_block(torch.cat((x1[:,3:,:,:], depth), dim=1) )  #sfv2 ç‚¹äº‘ + depthé¢„æµ‹sfï¼Œæ³¨æ„è¦æ”¹blocké‡Œé¢çš„in_channelæ•°é‡  

#         b, c, h, w = depth.size()
        
#         s = torch.sigmoid(sfmap[:,0,:,:]).unsqueeze(1) * 2.0  # sfv2æˆ–è€…ä¸ä½¿ç”¨p
#         # s = torch.ones_like(sfmap[:,0,:,:].unsqueeze(1))  # åŽ»æŽ‰s
        
        
#         # fç¼©æ”¾åˆ°-1 1
#         f = sfmap[:,1,:,:].unsqueeze(1)  # sfv2 æˆ–è€…ä¸é€‚ç”¨prob
#         # f = sfmap[:,0,:,:].unsqueeze(1)  # åŽ»æŽ‰s
#         f = torch.nn.functional.hardtanh(f, min_val= -0.5, max_val=0.5)  # sfv2æˆ–è€…ä¸é€‚ç”¨prob
#         # f = torch.zeros_like(sfmap[:,0,:,:].unsqueeze(1))  # ä¸ä½¿ç”¨f
        
        
#         # probæ˜¯æ¦‚çŽ‡ ä½¿ç”¨prob                                              
#         prob = self.softmax(sfmap[:,2,:,:]).unsqueeze(1)  # sfv2ï¼Œä½¿ç”¨p
#         # prob = self.softmax(sfmap[:,1,:,:]).unsqueeze(1)  # åŽ»æŽ‰s
#         # prob = torch.ones_like(f)  # ä¸ä½¿ç”¨p
#         # # ä½¿ç”¨G2æ—¶æ³¨é‡Š
#         depth = (depth*s+f)*prob + depth * (1-prob)
#         # prob = f
#         return depth,s,f,prob

class UNet(Module):
    def __init__(self, rgb_x_layer_num: int = 7, rezero: bool = False):
        super(UNet, self).__init__()
        # rgb+x channel
        layer = FirstModule(rgb_x_chan[1], rgb_x_chan[0], rezero)
        for i in range(2, rgb_x_layer_num):
            layer = UNetModule(layer, rgb_x_chan[i], rgb_x_chan[i - 1], rezero)

        self.rgb_x_block = Sequential(
            Conv2d(5, rgb_x_chan[i], 3, 1, 1),
            StackedBottleNeck(rgb_x_chan[i], rgb_x_chan[i], rezero),
            layer,
            StackedBottleNeck(2 * rgb_x_chan[i], rgb_x_chan[i], rezero),
            Conv2d(rgb_x_chan[i], 1, 3, 1, 1),
        )
        
        # sfmap channel
        sf_layer = FirstModule(sf_chan[1], sf_chan[0], rezero)
        for i in range(2, rgb_x_layer_num):
            sf_layer = UNetModule(sf_layer, sf_chan[i], sf_chan[i - 1], rezero)

        # # ä½¿ç”¨G2æ—¶æ³¨é‡Š
        self.sfmap_block = Sequential(
            Conv2d(3, sf_chan[i], 3, 1, 1),
            StackedBottleNeck(sf_chan[i], sf_chan[i], rezero),
            sf_layer,
            StackedBottleNeck(2 * sf_chan[i], sf_chan[i], rezero),
            Conv2d(rgb_x_chan[i], 3, 3, 1, 1),  #åŽŸå§‹sfv2
            # Conv2d(rgb_x_chan[i], 2, 3, 1, 1),  #ä¸ç”¨Probï¼Œsæˆ–f
        )
        
        self.softmax = nn.Softmax(dim=1)
        
        # initializing
        for m in self.modules():
            if isinstance(m, Conv2d):
                init.kaiming_normal_(m.weight, a=0.2)
                init.zeros_(m.bias)
        
 
    @autocast()
    def forward(
        self,
        rgb: Tensor,
        point: Tensor,
        hole_point: Tensor, 
    ) -> Tensor:

        x1 = torch.cat((rgb, point, hole_point), dim=1)
        depth = self.rgb_x_block(x1)
        
        # # ä½¿ç”¨G2æ—¶æ³¨é‡Š
        sfmap = self.sfmap_block(torch.cat((x1[:,3:,:,:], depth), dim=1) )  #sfv2 ç‚¹äº‘ + depthé¢„æµ‹sfï¼Œæ³¨æ„è¦æ”¹blocké‡Œé¢çš„in_channelæ•°é‡  

        b, c, h, w = depth.size()
        
        s = torch.sigmoid(sfmap[:,0,:,:]).unsqueeze(1) * 2.0  # sfv2æˆ–è€…ä¸ä½¿ç”¨p
        # s = torch.ones_like(sfmap[:,0,:,:].unsqueeze(1))  # åŽ»æŽ‰s
        
        
        # fç¼©æ”¾åˆ°-1 1
        f = sfmap[:,1,:,:].unsqueeze(1)  # sfv2 æˆ–è€…ä¸é€‚ç”¨prob
        # f = sfmap[:,0,:,:].unsqueeze(1)  # åŽ»æŽ‰s
        f = torch.nn.functional.hardtanh(f, min_val= -0.5, max_val=0.5)  # sfv2æˆ–è€…ä¸é€‚ç”¨prob
        # f = torch.zeros_like(sfmap[:,0,:,:].unsqueeze(1))  # ä¸ä½¿ç”¨f
        
        
        # probæ˜¯æ¦‚çŽ‡ ä½¿ç”¨prob                                              
        prob = self.softmax(sfmap[:,2,:,:]).unsqueeze(1)  # sfv2ï¼Œä½¿ç”¨p
        # prob = self.softmax(sfmap[:,1,:,:]).unsqueeze(1)  # åŽ»æŽ‰s
        # prob = torch.ones_like(f)  # ä¸ä½¿ç”¨p
        # # ä½¿ç”¨G2æ—¶æ³¨é‡Š
        dense_depth = depth.clone()
        depth = (depth*s+f)*prob + depth * (1-prob)
        # prob = f
        return depth,s,f,prob,dense_depth

class UNet_Visual_Only(UNet):
    def __init__(self, rgb_x_layer_num: int = 7, rezero: bool = False):
        super().__init__(rgb_x_layer_num, rezero)
        del self.sfmap_block
        del self.softmax
    @autocast()
    def forward(
        self,
        rgb: Tensor,
        point: Tensor,
        hole_point: Tensor, 
    ) -> Tensor:
        x1 = torch.cat((rgb, point, hole_point), dim=1)
        depth = self.rgb_x_block(x1)
        return (depth,) * 5  # è¿”å›ž6ä¸ªç›¸åŒdepthçš„tuple

# # ç›´æŽ¥å°†å‚æ•°é‡ç¿»å€
# class UNet(Module):
#     def __init__(self, rgb_x_layer_num: int = 7, rezero: bool = False):
#         super(UNet, self).__init__()
#         # rgb+x channel
#         layer = FirstModule(rgb_x_chan[1], rgb_x_chan[0], rezero)
#         for i in range(2, rgb_x_layer_num):
#             layer = UNetModule(layer, rgb_x_chan[i], rgb_x_chan[i - 1], rezero)

#         self.rgb_x_block = Sequential(
#             Conv2d(5, rgb_x_chan[i], 3, 1, 1),
#             StackedBottleNeck(rgb_x_chan[i], rgb_x_chan[i], rezero),
#             layer,
#             StackedBottleNeck(2 * rgb_x_chan[i], rgb_x_chan[i], rezero),
#             Conv2d(rgb_x_chan[i], 1, 3, 1, 1),
#         )
        
#         # sfmap channel
#         sf_layer = FirstModule(sf_chan[1], sf_chan[0], rezero)
#         for i in range(2, rgb_x_layer_num):
#             sf_layer = UNetModule(sf_layer, sf_chan[i], sf_chan[i - 1], rezero)

#         self.sfmap_block = Sequential(
#             Conv2d(3, sf_chan[i], 3, 1, 1),
#             StackedBottleNeck(sf_chan[i], sf_chan[i], rezero),
#             sf_layer,
#             StackedBottleNeck(2 * sf_chan[i], sf_chan[i], rezero),
#             Conv2d(rgb_x_chan[i], 1, 3, 1, 1),  #ç›´æŽ¥å°†å‚æ•°é‡ç¿»å€
#         )
        
#         # self.softmax = nn.Softmax(dim=1)
        
#         # initializing
#         for m in self.modules():
#             if isinstance(m, Conv2d):
#                 init.kaiming_normal_(m.weight, a=0.2)
#                 init.zeros_(m.bias)
        
 
#     @autocast()
#     def forward(
#         self,
#         rgb: Tensor,
#         point: Tensor,
#         hole_point: Tensor, 
#     ) -> Tensor:

#         x1 = torch.cat((rgb, point, hole_point), dim=1)
#         depth = self.rgb_x_block(x1)
#         x2 = torch.cat((depth, point, hole_point), dim=1)
#         output = self.sfmap_block(x2)
#         return output,output,output,output

# # ç›´æŽ¥å°†å‚æ•°é‡ç¿»å€_2
# class UNet(Module):
#     def __init__(self, rgb_x_layer_num: int = 7, rezero: bool = False):
#         super(UNet, self).__init__()
#         # rgb+x channel
#         layer = FirstModule(rgb_x_chan[1], rgb_x_chan[0], rezero)
#         for i in range(2, rgb_x_layer_num):
#             layer = UNetModule(layer, rgb_x_chan[i], rgb_x_chan[i - 1], rezero)

#         self.rgb_x_block = Sequential(
#             Conv2d(5, rgb_x_chan[i], 3, 1, 1),
#             StackedBottleNeck(rgb_x_chan[i], rgb_x_chan[i], rezero),
#             layer,
#             StackedBottleNeck(2 * rgb_x_chan[i], rgb_x_chan[i], rezero),
#             Conv2d(rgb_x_chan[i], 1, 3, 1, 1),
#         )
        
#         # sfmap channel
#         sf_layer = FirstModule(sf_chan[1], sf_chan[0], rezero)
#         for i in range(2, rgb_x_layer_num):
#             sf_layer = UNetModule(sf_layer, sf_chan[i], sf_chan[i - 1], rezero)

#         self.sfmap_block = Sequential(
#             Conv2d(1, sf_chan[i], 3, 1, 1),
#             StackedBottleNeck(sf_chan[i], sf_chan[i], rezero),
#             sf_layer,
#             StackedBottleNeck(2 * sf_chan[i], sf_chan[i], rezero),
#             Conv2d(rgb_x_chan[i], 1, 3, 1, 1),  #ç›´æŽ¥å°†å‚æ•°é‡ç¿»å€
#         )
        
#         # self.softmax = nn.Softmax(dim=1)
        
#         # initializing
#         for m in self.modules():
#             if isinstance(m, Conv2d):
#                 init.kaiming_normal_(m.weight, a=0.2)
#                 init.zeros_(m.bias)
        
 
#     @autocast()
#     def forward(
#         self,
#         rgb: Tensor,
#         point: Tensor,
#         hole_point: Tensor, 
#     ) -> Tensor:

#         x1 = torch.cat((rgb, point, hole_point), dim=1)
#         depth = self.rgb_x_block(x1)
#         output = self.sfmap_block(depth)
#         return output,output,output,output


# # redc
# class Unet(Module):
#     def __init__(self,rezero: bool = True):
#         super(Unet, self).__init__()
#         from baselines.ReDC.redc import ReDC
#         from baselines.ReDC.config import args as args_ReDC
#         self.network = ReDC(args_ReDC) 
        
#     def forward(self,
#         rgb: Tensor,
#         point: Tensor,
#         hole_point: Tensor, ):
#         fx, fy, cx, cy = 582.6244, 582.6910, 313.0447, 238.4438
#         K = torch.Tensor([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])
#         K = K.unsqueeze(0)
#         depth = self.network(rgb.cuda(), point.cuda(), K.cuda())  
#         return depth,depth,depth,depth

class V2Net(nn.Module):
    # ç”¨æ¥æµ‹SPNorm
    def __init__(self, depths=[3,3,27,3], dims=[192,384,768,1536], dp_rate=0.2, norm_type='CNX'):
        super(V2Net, self).__init__()

        self.visual_branch = nn.Sequential(
                Encoder(5, dims=dims, depths=depths, dp_rate=dp_rate, norm_type=norm_type),
                Decoder(1, dims=dims, norm_type=norm_type)
            )
        # initializing
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Conv2d) and hasattr(m, 'weight') and m.weight is not None:
            if m.weight.numel() > 0:
                nn.init.xavier_normal_(m.weight)
            else:
                print(f"Skipping Xavier initialization for {m} (zero-element tensor)")


    def forward(self, rgb, raw, hole_raw):
        x = torch.cat((rgb, raw, hole_raw), dim=1)
        # print(x.shape)
        # print(self.visual_branch)
        depth = self.visual_branch(x)
        
        return depth,depth,depth,depth